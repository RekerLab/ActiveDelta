# Imports
import os
import abc
import math
import shutil
import tempfile
import numpy as np
import pandas as pd
import chemprop
from rdkit import Chem
from rdkit.Chem import AllChem
from sklearn import metrics
from scipy import stats as stats
from sklearn.model_selection import KFold
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor as RF
from models import *

datasets = ['CHEMBL1075104-1',
'CHEMBL4153-1',
'CHEMBL4361-1',
'CHEMBL4361-2',
'CHEMBL4616-1',
'CHEMBL4792-1',
'CHEMBL4792-2',
'CHEMBL4794-1',
'CHEMBL4860-1',
'CHEMBL4908-1',
'CHEMBL4908-2',
'CHEMBL5071-1',
'CHEMBL5102-1',
'CHEMBL5112-1',
'CHEMBL5113-1',
'CHEMBL5113-2',
'CHEMBL302-1',
'CHEMBL313-1',
'CHEMBL313-2',
'CHEMBL318-1',
'CHEMBL318-2',
'CHEMBL344-1',
'CHEMBL344-2',
'CHEMBL3155-1',
'CHEMBL3227-1',
'CHEMBL3371-1',
'CHEMBL3510-1',
'CHEMBL3729-1',
'CHEMBL3759-1',
'CHEMBL3769-1',
'CHEMBL3837-1',
'CHEMBL3952-1',
'CHEMBL3952-2',
'CHEMBL3952-3',
'CHEMBL4005-1',
'CHEMBL4078-1',
'CHEMBL4105864-1',
'CHEMBL240-1',
'CHEMBL240-2',
'CHEMBL245-1',
'CHEMBL251-1',
'CHEMBL253-1',
'CHEMBL255-1',
'CHEMBL259-1',
'CHEMBL264-1',
'CHEMBL269-1',
'CHEMBL269-2',
'CHEMBL270-1',
'CHEMBL270-2',
'CHEMBL270-3',
'CHEMBL273-1',
'CHEMBL273-2',
'CHEMBL284-1',
'CHEMBL287-1',
'CHEMBL2492-1',
'CHEMBL2820-1',
'CHEMBL2954-1',
'CHEMBL222-1',
'CHEMBL222-2',
'CHEMBL223-1',
'CHEMBL224-1',
'CHEMBL224-2',
'CHEMBL225-1',
'CHEMBL225-2',
'CHEMBL228-1',
'CHEMBL228-2',
'CHEMBL229-1',
'CHEMBL229-2',
'CHEMBL231-1',
'CHEMBL232-1',
'CHEMBL233-1',
'CHEMBL234-1',
'CHEMBL236-1',
'CHEMBL237-1',
'CHEMBL238-1',
'CHEMBL2326-1',
'CHEMBL2366517-1',
'CHEMBL210-1',
'CHEMBL211-1',
'CHEMBL214-1',
'CHEMBL216-1',
'CHEMBL217-1',
'CHEMBL218-1',
'CHEMBL219-1',
'CHEMBL219-2',
'CHEMBL1800-1',
'CHEMBL1821-1',
'CHEMBL1833-1',
'CHEMBL1833-2',
'CHEMBL1862-1',
'CHEMBL1871-1',
'CHEMBL1889-1',
'CHEMBL1945-1',
'CHEMBL1946-1',
'CHEMBL2014-1',
'CHEMBL2035-1',
'CHEMBL2056-1',
'CHEMBL1293269-1',
'CHEMBL1908389-1']
 
models = [DeepDelta(), Trad_ChemProp(), Trad_RF(), Trad_XGB(), Delta_XGB(), 'Random_Selection']

random_split_datasets = ['Random_Split_For_AL_1', 'Random_Split_For_AL_2', 'Random_Split_For_AL_3']

# number of active learning iterations
start_iteration = 0
final_iteration = 200  

for random_split_dataset in random_split_datasets:

  for dataset in datasets:
      
    # Currently set to only run for the first random split of the Data
    training_dataframe = pd.read_csv("../Datasets/{}/{}_train_round_{}.csv".format(random_split_dataset, dataset, start_iteration))
    learning_dataframe = pd.read_csv("../Datasets/{}/{}_learning_round_{}.csv".format(random_split_dataset, dataset, start_iteration))

    for model in models:

      for iter in range(start_iteration, final_iteration):

        if str(model) != 'Random_Selection': # Using model prediction for selection

          ################
          ### Training ###
          ################

          training_dataframe = training_dataframe.reset_index(drop=True)
          x = training_dataframe[training_dataframe.columns[0]]
          y = training_dataframe[training_dataframe.columns[1]]

          model.fit(x,y) # Train model on the training dataset

          ##################
          ### Prediction ###
          ##################

          x_l_df = learning_dataframe[learning_dataframe.columns[0]]
          y_l_df = learning_dataframe[learning_dataframe.columns[1]]

          if str(model) == 'RandomForest' or str(model) == 'ChemProp50' or str(model) == 'XGBoost': # Traditional models 
            predictions = model.predict_single(x_l_df) # predict

          else: # ActiveDelta learning
            max_train_index = y.idxmax() # Get maximum training datapoint
            predictions = model.predict2(pd.DataFrame([x[max_train_index]]), x_l_df) # predict

          ##########################################
          ### Active Learning Molecule Selection ###
          ##########################################

          preds = pd.concat([y_l_df, predictions], axis=1)
          preds.columns =['Y', 'Y_Pred']
          preds['SMILES'] = learning_dataframe['SMILES'].tolist()

          # Add top prediction to training dataset
          SMILES = pd.DataFrame(preds.nlargest(1, 'Y_Pred')['SMILES'])
          TRUE = pd.DataFrame(preds.nlargest(1, 'Y_Pred')['Y'])
          PRED = pd.DataFrame(preds.nlargest(1, 'Y_Pred')['Y_Pred'])
          AL_ITER = pd.DataFrame({'Iteration': [iter+1]}, index=list(SMILES.index.values))
          top_molecule = pd.concat([SMILES, TRUE, PRED, AL_ITER], axis=1)
          training_dataframe = pd.concat([training_dataframe, top_molecule])

          # Remove top prediction from learning dataset
          learning_dataframe = pd.merge(learning_dataframe, top_molecule.drop(columns=['Y_Pred', 'Iteration']), on=['SMILES','Y'], how='outer', indicator=True).query("_merge != 'both'").drop('_merge', axis=1).reset_index(drop=True)

        else: # Meaning random selection of a datapoint
          random_molecule = learning_dataframe.sample()
          AL_ITER = pd.DataFrame({'Iteration': [iter+1]}, index=list(random_molecule.index.values))
          top_molecule = pd.concat([random_molecule, AL_ITER], axis=1)
          training_dataframe = pd.concat([training_dataframe, top_molecule])
          
          # Remove matching random molecule from learning dataset
          learning_dataframe = learning_dataframe.drop(random_molecule.index)

      # Save training dataframe
      training_dataframe.to_csv('{}_train_round_{}_{}_{}.csv'.format(dataset, str(model), iter + 1, random_split_dataset), index = False)
      learning_dataframe.to_csv('{}_learning_round_{}_{}_{}.csv'.format(dataset, str(model), iter + 1, random_split_dataset), index = False)
